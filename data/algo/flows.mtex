\title[] {Потоки}

\details[mode=main; summary=Постановка задачи] {
    Дан ориентированный граф из $V$ вершин и $E$ рёбер. В графе есть выделенная вершина $s$ (исток) и $t$ (сток). Каждое ребро задаётся тремя числами: $v$, $u$ и $c$~--- вершины, которые соединяет ребро, и его пропускная способность (от слова capacity).
    
    Надо найти максимальное число путей из $s$ в $t$ таких, что ребро $(v, u, c)$ содержится не больше чем в $c$ путях.
    
    Можно считать, что по рёбрам графа ходят люди и у каждого ребра есть ограничение на количество его использований. Надо найти максимальное количество людей, которые смогут дойти из вершины $s$ в вершину $t$.
    
    \details[summary=Почему это называется потоками?] {
        Смысл в том, что если представить, что рёбра~--- это трубы, то по каждому ребру можно пустить столько единиц жидкости, сколько путей через него проходит. Тогда для каждой вершины, кроме $s$ и $t$, будет выполняться, что количество втекающий в неё жидкости совпадает с количеством вытекающей. Это условие по сути означает, что жидкость в каждой вершине не накапливается.
        
        В такой формулировке, исток является бесконечным источником этой жидкости, а сток~--- бесконечным хранилищем. То есть в истоке жидкость появляется, а в стоке~--- скапливается.
        
        Описанные выше пути называются единичным потоком.
    }
}

\details[mode=main; summary=Идея решения] {
    Первая идея, которая может прийти в голову~--- жадник. То есть, пока существует хотя бы один путь из $s$ в $t$ по ещё не насыщенным рёбрам, выберем любой из них и увеличим поток каждого ребра на $1$.
    
    К сожалению, такой алгоритм не работает на следующем примере:
    
    \center[] {
        \image[height=200px; name=flows/graph-sample.png]{}
    }
    
    Красным выделен путь, который может быть найден жадником. Очевидно, что в этом случае можно выделить два пути, в то время как жадник найдёт только один.
    
    Оказывается, что данный жадник можно модифицировать: давайте для каждого ребра $v \to u$ исходного графа создадим обратное ребро $u \to v$ с пропускной способностью равной нулю. Если по прямому ребру течёт $x$ потока, то по обратному ребру будет течь $-x$ потока. То есть по обратным рёбрам может течь отрицательный поток (что означает, что поток на самом деле течёт в другую сторону). По прямым рёбрам отрицательный поток течь не может, такое мы не разрешаем.
    
    Теперь, при поиске пути жадным алгоритмом разрешим использовать в том числе ненасыщенные обратные рёбра (то есть рёбра, по которым течёт отрицательный поток). При нахождении очередного пути, надо увеличить величину потока по каждому ребру на $1$ и уменьшить величину потока по соответствующим обратным рёбрам. Обратим внимание на то, что прямое ребро так же является обратным ребром для его обратного ребра.
    
    Теперь, пример выше уже не ломает наш алгоритм, так как при нахождении такого пути появится ненасыщенное вертикальное ребро, из-за чего после этого найдётся такой путь:
    
    \center[] {
        \image[height=200px; name=flows/graph-sample-2.png]{}
    }
    
    Может возникнуть вопрос, почему наличие обратных рёбер ничего не ломает. На самом деле, для всех вершин, кроме $s$ и $t$, остаётся верен факт, что поток в них не накапливается, а значит исходная величина потока действительно каждый раз увеличивается на $1$, так как исходящий из истока и входящий в сток поток увеличивается на $1$.
    
    Оказывается, что данное улучшение жадника всегда найдёт максимальный поток. Доказательство этого описано далее.
}

\details[mode=main; summary=Разрезы] {
    Разрезом такого графа называется разделение его вершин на два множества $L$ и $R$, что вершина $s$ принадлежит $L$, а вершина $t$ принадлежит $R$.
    
    Величиной разреза называется сумма пропускных способностей рёбер $v \to u$, что левый конец ($v$) лежит в $L$, а правый конец ($u$) лежит в $R$. То есть это сумма пропускных способностей рёбер, идущих из $L$ в $R$.
    
    \center[] {
        <div>\image[width=350px; name=flows/cut-sample.png] {}</div>
        <h5>Красным отмечены рёбра, входящие в разрез.</h5>
    }

    Оказывается, что величина минимального разреза совпадает с величиной максимального потока. Не трудно показать, что поток не может быть больше любого разреза, однако данное утверждение более строгое. Доказательство данного утверждения описано далее.
    
    Разрез~--- это очень важная составляющая всей теории, связанной с потоками. Существует очень много задач, которые сводятся именно к поиску минимального разреза, а не максимального потока.
}

\details[mode=main; summary=Теорема Форда~--- Фалкерсона] {
    Следующие три условия эквивалентны:
    
    \enumerate[] {
        \item[] {
            Модифицированный жадный алгоритм, описанный выше, не нашёл увеличивающего пути.
        }
        \item[] {
            Существует насыщенный разрез. То есть существует разрез $(L, R)$, что все рёбра из $L$ в $R$ насыщены.
        }
        \item[] {
            В графе найден максимальный поток.
        }
    }
    
    \details[summary=Доказательство] {
        \details[summary=1 \implies 2] {
            Рассмотрим все достижимые из $s$ по ненасыщенным рёбрам вершины (обозначим можество этих вершин за $L$). Так как не существует увеличивающего пути, вершина $t$ не принадлежит $L$.
            
            Рассмотрим разрез $(L, R)$, где $R$~--- это соответственно все вершины, не принадлежащие $L$. Нетрудно видеть, что все рёбра из $L$ в $R$ насыщены, так как иначе существовала бы вершина из $R$, достижимая из $s$, что противоречит построению множества $L$.\qed
        }

        \details[summary=2 \implies 3] {
            Если мы рассмотрим любой разрез, то все пути потока должны проходить через рёбра из $L$ в $R$. Поэтому суммарное количество путей не может превосходить величину разреза.
            
            Таким образом, если все рёбра какого-то разреза насыщены, то поток максимален.\qed
        }

        \details[summary=3 \implies 1] {
            Если существует увеличивающий путь, то поток очевидно не максимальный, так как может быть увеличен.\qed
        }
    }
    
    Данная теорема, доказывает, что описанный выше модифицированный жадный алгоритм действительно найдёт максимальный поток.
    
    Легко заметить, что мы дополнительно доказали, что максимальный поток равен минимальному разрезу. Потому что выше мы доказали, что максимальный поток не может быть больше минимального разреза, а так же, что поток совпадает с каким-то разрезом, поэтому максимальный поток совпадает с минимальным разрезом.
}

\center[] {<h2>Алгоритм Форда~--- Фалкерсона (dfs)</h2>}

\details[mode=main; summary=Теория] {
    Предлагается делать ровно так же, как написано выше. Чтобы находить увеличивающий путь мы будем использовать dfs.
    
    Заметим, что пускать поток величины 1 по найденному пути не логично. Предлагается при нахождении пути, пустить не одну единицу, а максимально возможное количество, чтобы не переполнить ни одно из рёбер. Чтобы находить это количество можно прямо в dfs поддерживать ограничение сверху на поток, который можно пустить. Рекомендую посмотреть реализацию.
}

\details[mode=main; summary=Неориентированные рёбра] {
    Выше всегда предполагалось, что граф ориентированный. Но что если мы хотим научиться добавлять неориентированное ребро $(v, u, c)$? То есть поток может течь в любую сторону, но по величине не может превосходить $c$. Оказывается, что такие рёбра тоже легко добавить.
    
    До этого пропускную способность обратного ребра мы устанавливали равной нулю. Теперь давайте и у прямого ребра и у обратного ребра поставим пропускную способность равную $c$. Утверждается, что после этого можно спокойно запустить обычный алгоритм поиска максимального потока.
    
    Данный способ так же позволяет по-разному ограничивать поток в разные стороны. То есть, если надо, что бы от $v$ к $u$ величина потока не превосходила $c_1$, а от $u$ к $v$~--- не превосходила $c_2$, то у прямого ребра можно установить пропускную способность равной $c_1$, а у обратного~--- $c_2$.
    
    Так же можно делать по-другому: добавить два ориентированных ребра $(v \to u, c_1)$ и $(u \to v, c_2)$. Соответственно с учётом обратных рёбер всего получится 4 ребра. Тогда, пусть по первому ребру течёт $f_1$ потока, а по второму~--- $f_2$. Это соответствует тому, что по неориентированному ребру из $v$ в $u$ течёт $f_1 - f_2$ потока. Соответственно, если $f_1 - f_2 < 0$, то поток на самом деле течёт из $u$ в $v$. Нетрудно видеть, что $f_1 - f_2$ может принимать все значения от $-c_2$ до $c_1$ и только их, поэтому такая замена действительна эквивалентна исходной задачи.
    
    Обратим внимание, что ориентированное ребро $v \to u$ учитывается в разрезе $(L, R)$, если $v$ лежит в $L$, а $u$ лежит в $R$. Неориентированное ребро $(v, u)$ учитывается в разрезе, если $v$ и $u$ попали в разные множества.
}

\details[mode=main; summary=Время работы] {
    Данный алгоритм работает за \complexity$(F E)$, где $F$~--- максимальный поток, а $E$~--- количество рёбер. Так происходит, потому что каждый увеличивающий путь мы находим за \complexity$(E)$.
    
    Может показаться, что данный алгоритм работает быстрее, если мы разрешаем пушить не по одной единице потока, а сразу по несколько. К сожалению, это не так. Рассмотрим следующий пример:
    
    \center[] {
        \image[height=150px; name=flows/ff-counterexample.png] {}
    }
    
    Максимальный поток в этом графе равен $2F$. Если нам не повезёт, то dfs сначала найдёт путь $s \to 1 \to 2 \to t$, после чего найдёт путь $s \to 2 \to 1 \to t$. Так он будет делать $F$ раз.
    
    Можно попытаться каждый раз шафлить список смежности, но лучше, конечно, писать доказуемо более быстрые алгоритмы.
}

\details[mode=main; summary=Реализация] {
\code[language=C++] {
/*
 * T is a capacity integral type.
 * An implementation of the Ford–Fulkerson algorithm in O(flow * E).
 */
template<typename T>
class max_flow {
private:
    struct edge {
        int to, rev;
        T capacity, flow;
    };

    int n;
    std::vector<std::vector<edge>> g;
    std::vector<int> used;
    int timer, source, sink;

    T dfs(int v, T f) {
        if (used[v] == timer || f == 0) {
            return 0;
        }
        if (v == sink) {
            return f;
        }
        used[v] = timer;
        for (auto &[u, rev, capacity, flow] : g[v]) {
            if (T delta = dfs(u, std::min(f, capacity - flow))) {
                flow += delta;
                g[u][rev].flow -= delta;
                return delta;
            }
        }
        return 0;
    }

public:
    max_flow(int n = 0) : n(n), g(n) {}

    int add(int from, int to, T forward_capacity, T backward_capacity = 0) {
        int id = g[from].size();
        g[from].push_back({to, int(g[to].size()) + (from == to), forward_capacity, 0});
        g[to].push_back({from, id, backward_capacity, 0});
        return id;
    }

    // U is a max flow integral type.
    template<typename U>
    U solve(int source_, int sink_) {
        source = source_, sink = sink_;
        timer = 0;
        used.assign(n, -1);
        U flow = 0;
        while (T delta = dfs(source, std::numeric_limits<T>::max())) {
            flow += delta;
            timer++;
        }
        return flow;
    }
};
}
}

\center[] {<h2>Алгоритм Эдмондса~--- Карпа (bfs)</h2>}

\details[mode=main; summary=Теория] {
    Предлагается просто вместо dfs использовать bfs. То есть каждый раз находить любой кратчайший (по количеству рёбер) путь из $s$ в $t$ и увеличивать поток на нём аналогично алгоритму Форда~--- Фалкерсона
}

\details[mode=main; summary=Время работы] {
    Если алгоритм с dfs'ом был неполиномиальным, то данная реализация уже будет работать за \complexity$(VE^2)$.
    
    В прошлом алгоритме проблема была в том, что каждое ребро потенциально могло насытиться \complexity$(F)$ раз. То есть если ребро насытилось, то из-за возможности протолкнуть поток по соответствующему ему обратному ребру, оно могло опять стать ненасыщенным. В данном случае получается так, что каждое ребро может насытиться не больше \complexity$(V)$ раз.
    
    \details[summary=Доказательство] {
        Сначала заметим, что минимальное расстояние до каждой вершины не может уменьшиться после очередного проталкивания потока. Для того, чтобы это доказать, поймём какие рёбра могли перестать быть насыщенными после очередного проталкивания. Нетрудно видеть, что это могли быть только обратные рёбра к тем, по которым мы пустили поток. Но обратные рёбра идут из вершины с большим расстоянием к вершине с меньшим расстоянием. Значит не могло появиться ненасыщенных рёбер, которые могут \<<сократить\>> путь до какой-либо вершины.
        
        То есть если до проталкивания потока расстояние до вершины $v$ было равно $d(v)$, то после проталкивания не могло появиться ненасыщенных рёбер, ведущих из вершины $x$ в $y$, что $d(y) > d(x) + 1$. Таким образом, кратчайшее расстояние до вершины $v$ не могло стать меньше $d(v)$.
        
        Теперь рассмотрим ребро $v \to u$. Предположим, что оно насытилось, когда кратчайшее расстояние до $v$ было равно $d$. По определению нашего алгоритма, кратчайшее расстояние до $u$ в этот момент было равно $d + 1$. Чтобы это ребро можно было насытить ещё раз, надо пустить поток по соответствующему обратному ребру $u \to v$. Но это может произойти, только если кратчайшее расстояние до $v$ стало на 1 больше, чем до $u$. Но мы показали, что кратчайшее расстояние не может уменьшаться, поэтому в следующий раз, ребро может насытиться только если кратчайшее расстояние до $v$ стало не меньше чем $d + 2$.
        
        Таким образом, каждое ребро действительно может насытиться не больше чем \complexity$(V)$ раз, так как расстояние сверху ограничено числом $V$.\qed
    }
    
    Таким образом, суммарно может быть не больше \complexity$(VE)$ насыщений. Каждый запуск bfs насыщает хотя бы одно ребро и работает за \complexity$(E)$. То есть весь алгоритм работает не хуже чем за \complexity$(VE^2)$.
}

\details[mode=main; summary=Реализация] {
\code[language=C++] {
/*
 * T is a capacity integral type.
 * An implementation of the Edmonds–Karp algorithm in O(VE^2).
 */
template<typename T>
class max_flow {
private:
    struct edge {
        int to, rev;
        T capacity, flow;
    };

    int n;
    std::vector<std::vector<edge>> g;
    std::vector<T> f;
    std::vector<std::pair<int, int>> par;
    int source, sink;

    T bfs() {
        par.assign(n, {-1, -1});
        f.assign(n, 0);
        std::vector<int> que{source};
        f[source] = std::numeric_limits<T>::max();

        for (int ptr = 0; ptr < int(que.size()); ptr++) {
            int v = que[ptr];
            for (const auto &[u, rev, capacity, flow] : g[v]) {
                if (par[u].first == -1 && flow < capacity) {
                    par[u] = {v, rev};
                    f[u] = std::min(f[v], capacity - flow);
                    que.push_back(u);
                }
            }
        }
        return f[sink];
    }

public:
    max_flow(int n = 0) : n(n), g(n) {}

    int add(int from, int to, T forward_capacity, T backward_capacity = 0) {
        int id = g[from].size();
        g[from].push_back({to, int(g[to].size()) + (from == to), forward_capacity, 0});
        g[to].push_back({from, id, backward_capacity, 0});
        return id;
    }

    // U is a max flow integral type.
    template<typename U>
    U solve(int source_, int sink_) {
        source = source_, sink = sink_;
        U flow = 0;
        while (T delta = bfs()) {
            flow += delta;
            for (int v = sink; v != source; v = par[v].first) {
                auto [u, i] = par[v];
                g[v][i].flow -= delta;
                g[u][g[v][i].rev].flow += delta;
            }
        }
        return flow;
    }
};
}
}

\center[] {<h2>Алгоритм Диница (bfs + dfs)</h2>}

\details[mode=main; summary=Теория] {
    Алгоритм Диница совмещает алгоритм Форда~--- Фалкерсона и Эдмондса~--- Карпа, добиваясь лучшей асимптотики.
    
    Одина итерация алгоритма Диница выглядит таким образом:
    
    \enumerate[] {
        \item[] {
            Запускаем bfs из вершины $s$ по ещё не насыщенным рёбрам. Если путь до $t$ не найден, то найден максимальный поток и алгоритм завершается.
        }
        \item[] {
            Иначе, запускаем аналогичный алгоритму Форда~--- Фалкерсона dfs, в котором разрешаем использовать только ненасыщенные рёбра $v \to u$, что $d(v) + 1 = d(u)$, где $d(x)$~--- минимальное (по количеству рёбер) расстояние от $s$ до $x$, найденное ранне bfs'ом.
        }
    }
    
    Если написать алгоритм прямо так, то время работы всё ещё будет \complexity$(VE^2)$. Чтобы улучшить асимптотику заметим следующее: во время одной итерации не может появиться новых ненасыщенных рёбер $x \to y$, что $d(x) + 1 = d(y)$. Поэтому, если в какой-то момент внутри dfs мы попытались увеличить поток по ребру, но сделать этого не получилось~--- значит далее тоже никогда не получится увеличить поток по данному ребру и его можно больше не рассматривать.
    
    Чтобы реализовать данную оптимизацию, для каждой вершины $v$ можно хранить указатель на первое ребро в её списке смежности, по которому ещё потенциально можно увеличить поток. Если по ребру не получилось пустить поток, то указатель надо увеличить.
    
    Оказывается, что данная оптимизация влияет на асимптотику алгоритма.
}

\details[mode=main; summary=Время работы] {
    Заметим, что из-за того, что на каждой итерации не может добавиться рёбер $x \to y$, что $d(x) + 1 = d(y)$, минимальное расстояние от $s$ до $t$ увеличивается хотя бы на $1$ каждую итерацию. Таким образом, алгоритм Диница сделает не больше $V$ итераций.
    
    Теперь докажем, что одна итерация алгоритма Диница работает за \complexity$(VE)$. Запуск bfs работает за \complexity$(E)$ и на асимптотику итерации не влияет. С dfs немного сложнее. Внутри каждого dfs происходит следующее:
    
    \itemize[] {
        \item[] {
            Если при переборе очередного ребра поток пустить не получилось, то мы увеличиваем указатель на один. Всего увеличений \complexity$(E)$, поэтому данный случае не влияет на асимптотику.
        }
        \item[] {
            Если поток получилось протолкнуть, то хотя бы одно ребро насытилось, из-за чего хотя бы один указатель увеличится. При этом часть указателей останется на своём месте. Но поскольку длина любого пути не превосходит $V$, значение потока в рёбрах изменится за \complexity$(V)$. То есть за каждое увеличения указателя мы в худщем случае заплатим \complexity$(V)$. А так как увеличений указателя \complexity$(E)$ данная часть суммарно работает за \complexity$(VE)$.
        }
    }
    
    Таким образом, алгоритм Диница работает за \complexity$(V \cdot VE) =$ \complexity$(V^2E)$.
}

\details[mode=main; summary=Минимальный разрез] {
    В теореме Форда~--- Фалкерсона мы показали, что после нахождения максимального потока найдётся насыщенный разрез, который так же будет минимальным.
    
    Для нахождения минимального разреза достаточно, например, взять в качестве множества $L$ все вершины достижимые из $s$ по ненасыщенным рёбрам (при том не важно, учитываем мы обратные рёбра или нет). Заметим, что алгоритм Диница закончится когда bfs не найдёт путь из $s$ в $t$. Но этот же bfs, как раз сможет посетить все вершины, достижимые из $s$. Поэтому для нахождения какого-то минимального разреза можно не запускать отдельный dfs, достаточно переспользовать уже насчитанные значения.
    
    Минимальные разрезы так же обладают таким свойством: если разрезы $(L_1, R_1)$ и $(L_2, R_2)$ минимальны, то разрезы $(L_1 \cap L_2, V\ \backslash\ (L_1 \cap L_2))$ и $(L_1 \cup L_2, V\ \backslash\ (L_1 \cup L_2))$ тоже минимальные.
    
    \details[summary=Доказательство] {
        Давайте пустим по рёбрам максимальный поток.

        Из минимальности разреза $(L, R)$ следует, что все рёбра из $L$ в $R$ насыщены. Таким образом, все рёбра из $L_1 \cap L_2$ в остальные вершины тоже насыщены. Аналогично все рёбра из $L_1 \cup L_2$ в остальные вершины тоже насыщены. Из насыщенности всех рёбер в разрезе следует его минимальность.\qed
    }
    
    Используя этот факт, можно, например, проверять минимальный разрез на единственность. Для этого надо всего лишь проверить, что минимальный по вложенности левой доли разрез совпадает с максимальным по вложенности разрезом. Чтобы найти минимальный по вложенности разрез достаточно найти найти достижимые из $s$ вершины, а чтобы найти максимальный достаточно найти вершины, из которых достижима вершина $t$.
}

\details[mode=main; summary=Декомпозиция потока] {
    После запуска любого описанного выше алгоритма мы явно не находим его декомпозицию. Восстанавливать искомые пути надо отдельно.
    
    Формально, декомпозиция потока~--- это его разбиение на пути и циклы. Дело в том, что только на пути разбить найденный поток может не получиться, так как циклы так же не ломают инвариант про сумму входящего и исходящего потока. Например, может существовать изолированный цикл, по которому циркулирует поток.
    
    Нас интересуют только пути, а не циклы, поэтому научимся выделять именно их. Алгоритм будет очень простой: пока существует путь из $s$ в $t$ по рёбрам, по которым течёт положительный поток: уменьшаем поток на этом пути и продолжаем алгоритм. Обратим внимание, что сейчас мы уже не рассматриваем обратные рёбра, хотя формально они не противоречат алгоритму, так как по ним не может течь положительный поток.
    
    Понятно, что мы можем выделить пути в сжатом виде, то есть уменьшать поток по найденому пути не на $1$, а на величину минимального потока, текущего по рёбрам этого пути.
    
    Такой алгоритм, если реализовать его через dfs аналогично Диницу, работает за \complexity$(VE)$. Доказательство опущено, потому что оно полностью аналогично доказательству времени работы Диница.
    
    Заметим, что количество путей, которые мы выделим совпадает с размером потока, так как пока из истока исходит ненулевой поток, алгоритм обязательно найдёт путь из истока в сток, как раз из-за того, что поток не накапливается.
}

\details[mode=main; summary=Реализация] {
\code[language=C++] {
/*
 * T is a capacity integral type.
 * An implementation of the Dinic algorithm in O(V^2E).
 */
template<typename T>
class max_flow {
public:
    struct edge {
        int to, rev;
        T capacity, flow;
    };

    struct path {
        T flow;
        std::vector<int> path, edges;
    };

private:
    int n;
    std::vector<std::vector<edge>> g;
    std::vector<int> dist, head;
    int source, sink;

    bool bfs() {
        dist.assign(n, n);
        dist[source] = 0;
        std::vector<int> que{source};
        for (int ptr = 0; ptr < int(que.size()); ptr++) {
            int v = que[ptr];
            for (const auto &[u, rev, capacity, flow] : g[v]) {
                if (dist[u] > dist[v] + 1 && flow < capacity) {
                    dist[u] = dist[v] + 1;
                    que.push_back(u);
                }
            }
        }
        return dist[sink] != n;
    }

    T dfs(int v, T f) {
        if (v == sink) {
            return f;
        }
        for (; head[v] < int(g[v].size()); head[v]++) {
            auto &[u, rev, capacity, flow] = g[v][head[v]];
            if (dist[u] == dist[v] + 1 && flow < capacity) {
                if (T delta = dfs(u, std::min(f, capacity - flow))) {
                    flow += delta;
                    g[u][rev].flow -= delta;
                    return delta;
                }
            }
        }
        return 0;
    }

    T decompose_dfs(int v, T f, std::vector<int> &path, std::vector<int> &edges) {
        if (v == sink) {
            path.push_back(sink);
            return f;
        }
        for (; head[v] < int(g[v].size()); head[v]++) {
            auto &[u, rev, capacity, flow] = g[v][head[v]];
            if (flow > 0) {
                if (T delta = decompose_dfs(u, std::min(f, flow), path, edges)) {
                    flow -= delta;
                    g[u][rev].flow += delta;
                    path.push_back(v);
                    edges.push_back(head[v]);
                    return delta;
                }
            }
        }
        return 0;
    }

public:
    max_flow(int n = 0) : n(n), g(n) {}

    int size() const {
        return n;
    }

    /*
     * Returns an index of the added edge.
     * To get information about the edge use get_edge(from, index).
     */
    int add(int from, int to, T forward_capacity, T backward_capacity = 0) {
        int id = g[from].size();
        g[from].push_back({to, int(g[to].size()) + (from == to), forward_capacity, 0});
        g[to].push_back({from, id, backward_capacity, 0});
        return id;
    }

    // Returns i-th edge of vertex v.
    edge get_edge(int v, int id) const {
        return g[v][id];
    }

    void clear_flow() {
        for (int v = 0; v < n; v++) {
            for (auto &e : g[v]) {
                e.flow = 0;
            }
        }
    }

    /*
     * U is a max flow integral type.
     ! If source and sink are different from previous call of solve, then call clear_flow() before.
     */
    template<typename U>
    U solve(int source_, int sink_) {
        source = source_, sink = sink_;
        U flow = 0;
        while (bfs()) {
            head.assign(n, 0);
            while (T delta = dfs(source, std::numeric_limits<T>::max())) {
                flow += delta;
            }
        }
        return flow;
    }

    /*
     * Returns true if v is on the left side of the mincut.
     ! Require: solve must be called before using it.
     */
    bool left_of_mincut(int v) const {
        return dist[v] != n;
    }

    /*
     * Returns list of (flow, path, edges) where:
        - 'flow' is the flow value of the path.
        - 'path' is the list of vertices (path[0] == source && path.back() == sink).
        - 'edges' is the list of edge indices of the path returned by add method.
     ! Require: solve must be called before using it.
     ! WARNING: invalidates the flow values.
     */
    std::vector<path> decompose() {
        std::vector<path> paths;
        std::vector<int> path, edges;
        head.assign(n, 0);
        while (T flow = decompose_dfs(source, std::numeric_limits<T>::max(), path, edges)) {
            std::reverse(path.begin(), path.end());
            std::reverse(edges.begin(), edges.end());
            paths.push_back({flow, path, edges});
            path.clear();
            edges.clear();
        }
        return paths;
    }
};
}
}

\details[mode=main; summary=Оценки Карзанова] {
    Введём следующие обозначения:
    
    \itemize[] {
        \item[] {
            $\mathrm{out}(v)$~--- сумма пропускных способностей всех рёбер, исходящих из $v$. То есть это оценка сверху на максимальный поток, вытекающий из вершины $v$.
        }
        
        \item[] {
            $\mathrm{in}(v)$~--- сумма пропускных способностей всех рёбер, входящих в $v$. То есть это оценка сверху на максимальный поток втекающий в вершину $v$.
        }
        
        \item[] {
            $c(v)$~--- это оценка сверху на поток, который может пройти через вершину $v$. Для стока и истока обозначим $c(v)$ следующим образом: $c(s) = \mathrm{out}(s)$, $c(t) = \mathrm{in}(t)$. Для всех остальных вершин скажем, что $c(v) = \min(\mathrm{in}(v), \mathrm{out}(v))$.
        }
        
        \item[] {
            $C = \sum c(v)$. То есть $C$~--- это сумма ограничений сверху на поток, протекающей по каждой из вершин.
        }
        
        \item[] {
            $W$~--- максимальная пропускная способность по всем рёбрам.
        }
    }

    <b>Оценка 1.</b> Количество итераций алгоритма Диница не превосходит \complexity$(\sqrt{C})$. Таким образом, алгоритм диница работает за \complexity$(VE\sqrt{C})$.
    
    \details[summary=Доказательство] {
        Заметим, что после первых $\sqrt{C}$ итераций, расстояние от $s$ до $t$ будет не меньше $\sqrt{C}$. Каждая итерация Диница, насыщает хотя бы один путь, значит последующие итерации будут затрагивать хоят бы $\sqrt{C}$ вершин. Но вершина $v$ может быть затронута не больше $c(v)$ раз, так как это оценка сверху на протекающий через неё поток, а каждый раз когда она лежит на насыщающем пути, поток через неё увеличивается. Таким образом, суммарная длина путей, которые найдёт алгоритм Диница не может быть больше $C$. А так как, длина каждого пути после $\sqrt{C}$ итераций не меньше $\sqrt{C}$, количество оставшихся итераций не больше чем $\frac{C}{\sqrt{C}} = \sqrt{C}$.\qed
    }
    
    <b>Следствие.</b> Если пропускные способности всех рёбер равны $1$, то алгоритм Диница работает за \complexity$(E\min(V, \sqrt{C}))$.
    
    \details[summary=Доказательство] {
        Каждая итерация алгоритма Диница на единичных сетях работает за \complexity$(E)$. Так происходит, потому что по каждому ребру алгоритм пройдёт только один раз из-за того, что оно после этого становится насыщеным или далее оно не будет рассмотрено алгоритмом из-за того, что по нему не получилось пустить поток. Мы показали, что всего итераций будет не больше \complexity$(\sqrt{C})$, поэтому время работы можно оценить, как \complexity$(E\min(V, \sqrt{C}))$.\qed
    }
    
    Если искать максимальное паросочетание с помощью алгоритма Диница, то в построенном графе $C =$ \complexity$(V)$, то есть алгоритм будет работать за \complexity$(E\sqrt{V})$. В теории это лучше, чем Кун, но на практике Кун всё равно работает быстрее.
    
    <b>Оценка 2.</b> Если в графе нет кратных рёбер, то количество итераций алгоритма Диница не превосходит \complexity$(W^{\frac{1}{3}}V^{\frac{2}{3}})$. Таким образом, алгоритм диница работает за \complexity$(V^{\frac{5}{3}}EW^{\frac{1}{3}})$.
    
    \details[summary=Доказательство] {
        После первых $k=W^{\frac{1}{3}}V^{\frac{2}{3}}$ расстояние от $s$ до $t$ будет больше чем $k$. Рассмотрим следующую итерацию. Разобьём вершины на слои по расстоянию от $s$. Всего слоёв больше чем $k$. Пусть суммарно на $i$-м и $(i+1)$-м слое $x_i$ вершин. Тогда заметим, что:
        $$
        \sum\limits_{i=1}^k x_i \le 2n \implies
        \min x_i \le \frac{2V}{k}
        $$
        Таким образом, найдётся пара соседних слоёв, что на них суммарно не больше $p = \frac{2V}{k}$ вершин. Так как в графе нет кратных рёбер, количество рёбер между этими слоями не превосходит:
        $$
        \Big(\frac{p}{2}\Big)^2 =
        \frac{V^2}{W^{\frac{2}{3}} V^{\frac{4}{3}}} = \frac{V^{\frac{2}{3}}}{W^{\frac{2}{3}}}
        $$
        Значит, разрез, проходящий между данными слоями имеет величину не больше чем:
        $$
        \frac{V^{\frac{2}{3}}}{W^{\frac{2}{3}}} \cdot W = W^{\frac{1}{3}}V^{\frac{2}{3}} = k
        $$
        То есть оставшийся поток в графе не превосходит этой величины, а значит и количество оставшихся итераций не превосходит этой величины.\qed
    }
}

\center[] {<h2>Масштабирование потока</h2>}

\details[mode=main; summary=Теория] {
    Пусть все пропускные способности строго меньше $2^k = W$. Предлагается перебрать все степени двойки $D$ от $2^{k-1}$ до $2^0 = 1$ и использовать любой из приведённых выше алгоритмов с одним изменением: теперь мы будем рассматривать не все ненасыщенные рёбра, а только те рёбра по которым ещё можно пустить хотя бы $D$ потока. Далее, пока можно пустить хотя бы $D$ потока по какому-либо пути, мы будем его пускать. Как только этого сделать не получилось, перейдём к следующей степени двойки (то есть уменьшим $D$ в два раза).
    
    Так как при $D=1$ наш алгоритм выродится в один в алгоритм без масштабирования, данный способ работает корректно.
    
    Оказывается, что если граф, на котором мы ищем поток может быть достаточно произвольным, данный способ работает быстрее, чем описанные выше.
}

\details[mode=main; summary=Время работы] {
    Если не существует пути из $s$ в $t$ по которому можно пустить хотя бы $D$ потока, то существует разрез $(L, R)$, что по всем рёбрам из $L$ в $R$ можно пустить не больще $D-1$ потока. Значит оставшийся поток не превосходит $E(D - 1)$. Это значит, что когда мы начинаем итерацию со значением $D$, можно пустить поток по не больше чем по $\frac{E(2D - 1)}{D} = $ \complexity$(E)$ путям.
    
    Если для поиска путей использовать Форда~--- Фалкерсона или Эдмондса~--- Карпа, то каждый такой путь будет найден за \complexity$(E)$, то есть весь алгоритм будет работать за \complexity$(E^2 \log W)$.
    
    Если использовать алгоритм Диница, то для каждого значения $D$ насыщений будет \complexity$(E)$. В доказательстве самого алгоритма мы оценивали количество насыщений, как \complexity$(VE)$, из-за чего асимптотика и получалась \complexity$(V^2E)$, так как на каждое насыщение работает за \complexity$(V)$ (так как надо пройтись по всему пути). То есть здесь эта часть уже работает за \complexity$(VE)$. Все остальные составляющие алгоритма уже работают за \complexity$(VE)$, так как на каждой итерации каждое ребро может не насытиться не более одного раза.
    
    Таким образом, если использовать алгоритм Диница, это будет работать за \complexity$(VE)$ для каждого $D$, то есть суммарно это будет работать за \complexity$(VE\log W)$.
}

\details[mode=main; summary=Реализация] {
\code[language=C++] {
/*
 * T is a capacity integral type.
 * An implementation of the Dinic algorithm with flow scaling in O(VE log(C)).
 */
template<typename T>
class max_flow {
public:
    struct edge {
        int to, rev;
        T capacity, flow;
    };

private:
    int n;
    std::vector<std::vector<edge>> g;

    std::vector<int> dist, head;
    int source, sink;
    T smallest;

    bool bfs() {
        dist.assign(n, n);
        dist[source] = 0;
        std::vector<int> que{source};
        for (int ptr = 0; ptr < int(que.size()); ptr++) {
            int v = que[ptr];
            for (const auto &[u, rev, capacity, flow] : g[v]) {
                if (dist[u] > dist[v] + 1 && capacity - flow >= smallest) {
                    dist[u] = dist[v] + 1;
                    que.push_back(u);
                }
            }
        }
        return dist[sink] != n;
    }

    T dfs(int v, T min_delta) {
        if (v == sink) {
            return min_delta;
        }
        for (; head[v] < int(g[v].size()); head[v]++) {
            auto &[u, rev, capacity, flow] = g[v][head[v]];
            if (dist[u] == dist[v] + 1 && capacity - flow >= smallest) {
                if (T delta = dfs(u, std::min(min_delta, capacity - flow))) {
                    flow += delta;
                    g[u][rev].flow -= delta;
                    return delta;
                }
            }
        }
        return 0;
    }

public:
    max_flow(int n = 0) : n(n), g(n) {}

    int size() const {
        return n;
    }

    int add(int from, int to, T forward_capacity, T backward_capacity = 0) {
        int id = g[from].size();
        g[from].push_back({to, int(g[to].size()) + (from == to), forward_capacity, 0});
        g[to].push_back({from, id, backward_capacity, 0});
        return id;
    }

    // U is a max flow integral type.
    template<typename U>
    U solve(int source_, int sink_) {
        source = source_, sink = sink_;
        U flow = 0;
        for (smallest = T(1) << std::__lg(std::numeric_limits<T>::max()); smallest >= 1; smallest >>= 1) {
            while (bfs()) {
                head.assign(n, 0);
                while (T delta = dfs(source, std::numeric_limits<T>::max())) {
                    flow += delta;
                }
            }
        }
        return flow;
    }
};
}
}

\center[] {<h2>Приложения</h2>}

\details[mode=main; summary=Задачи] {
    \item[] {\link[href=https://atcoder.jp/contests/arc161/tasks/arc161_f]{Atcoder ARC161-F}}
    \item[] {\link[href=https://codeforces.com/contest/1473/problem/F]{Codeforces 1473-F}}
    \item[] {\link[href=https://codeforces.com/gym/102896/problem/O]{Northern Eurasia Finals Online 2020. Задача O}}
    \item[] {\link[href=https://codeforces.com/contest/1666/problem/K]{2021-2022 ICPC, NERC, Northern Eurasia. Задача K}}
}
